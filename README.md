# qiushi_history_spider
# 项目详细介绍：http://www.cnblogs.com/GISRSMAN/p/6245401.html
这是写给自己玩的练习项目，从糗事百科中爬取段子放到微信公众号上去，这样我就能随时随地的看段子了，啊哈哈哈

这是完整项目的第一部分：Python爬虫，糗事百科段子抓取部分

技术实现：Python2.7

插件使用：re,urllib2,pymysql。没有使用beatifulsoap

实现思路：

1）起始：从糗事百科的“穿越”（http://www.qiushibaike.com/history/）栏，可以随机打开之前的某一天的糗事页面

2）查重：根据日期判断数据库中是否已经抓取过此页面

3）页面下载：如果没有抓去过，则下载当天前两个页面到页面队列中，等待解析

4）解析页面：糗事、页面url、日期、下一天的url（页面中随机展示）

5）持久化存储：将分离出来的逐个段子和其他信息，存储到mysql中

6）下一轮抓取：根据4中得到的下一页url，进入2流程（此步骤也可以是不断刷新“穿越”页面得到新的页面，即进入1流程）

7）数据输出：为了能在新浪云SAE中使用段子数据（免费云空间不能使用mysql），从mysql中导出糗事段子为文本文件
